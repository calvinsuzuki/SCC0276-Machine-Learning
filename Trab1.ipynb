{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bbd3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mne.viz import plot_alignment, set_3d_view\n",
    "# from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a03ca",
   "metadata": {},
   "source": [
    "# Exploring CHB-MIT dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa7f2e",
   "metadata": {},
   "source": [
    "## Reading edf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed456a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "patient_id = 1\n",
    "DATASET_PATH = os.path.join(os.getcwd(), 'data', 'chb01-summary.txt') #ONLY FOR TESTING\n",
    "patient_id_str = str(patient_id).zfill(2)\n",
    "\n",
    "all_file_path = glob(f'chb{patient_id_str}/*.edf')\n",
    "print(len(all_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658df51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/guisoares/soares_repo/SCC0276-Machine-Learning/chb01/chb01_03.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "['eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg', 'eeg']\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: FP1-F7, F7-T7, T7-P7, P7-O1, FP1-F3, F3-C3, C3-P3, P3-O1, ...\n",
      " chs: 23 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 128.0 Hz\n",
      " meas_date: 2076-11-06 13:43:04 UTC\n",
      " nchan: 23\n",
      " projs: []\n",
      " sfreq: 256.0 Hz\n",
      ">\n",
      "['FP1-F7', 'F7-T7', 'T7-P7', 'P7-O1', 'FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1', 'FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2', 'FP2-F8', 'F8-T8', 'T8-P8-0', 'P8-O2', 'FZ-CZ', 'CZ-PZ', 'P7-T7', 'T7-FT9', 'FT9-FT10', 'FT10-T8', 'T8-P8-1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22045/2924192598.py:1: RuntimeWarning: Channel names are not unique, found duplicates for: {'T8-P8'}. Applying running numbers for duplicates.\n",
      "  raw = mne.io.read_raw_edf(all_file_path[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 921600)\n"
     ]
    }
   ],
   "source": [
    "raw = mne.io.read_raw_edf(all_file_path[0])\n",
    "print(raw.get_channel_types())\n",
    "print(raw.info)\n",
    "print(raw.ch_names)\n",
    "print(raw.get_data().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cee985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_edf(file_path):\n",
    "    data = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    data.set_eeg_reference()\n",
    "    data.filter(l_freq=0.5, h_freq=45)\n",
    "    epochs = mne.make_fixed_length_epochs(data, duration=10, overlap=1)\n",
    "    epochs_array = epochs.get_data()\n",
    "    return epochs, epochs_array\n",
    "# def read_data_seizures(file_path):\n",
    "#     ref_file = open(file_path, 'r')\n",
    "#     array=epochs.get_data()\n",
    "#     return array\n",
    "\n",
    "def read_edf_to_raw(file_path):\n",
    "    raw = mne.io.read_raw_edf(file_path, preload=True)\n",
    "    raw.set_eeg_reference()\n",
    "    raw.filter(l_freq=0.5, h_freq=45)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21e717ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Shape ()\n",
    "epochs_data_list = [read_data_edf(i)[1] for i in all_file_path]\n",
    "epochs_list = [read_data_edf(i)[0] for i in all_file_path]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d11b6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 399, 23, 2560)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs_data_array = np.array(epochs_data_list)\n",
    "epochs_data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ffec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Each file has a representant epochs objects that contains all the epochs.  \n",
    "print(epochs_list[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa67ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1596, 23, 2560)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the files on one array \n",
    "# shape: (N_files, 399 epochs, 23 channels, 2560 values) ->  (N_files*399 epochs, 23 channels, 2560 values)\n",
    "data_array = np.vstack(epochs_data_array)\n",
    "data_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728472b",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "680dcfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "########! Features extraction !#####\n",
    "from scipy import stats\n",
    "def mean(x):\n",
    "    return np.mean(x, axis=-1)\n",
    "def std(x):\n",
    "    return np.std(x, axis=-1)\n",
    "def ptp(x):\n",
    "    return np.ptp(x, axis=-1)\n",
    "def var(x):\n",
    "    return np.var(x, axis=-1)\n",
    "def minim(x):\n",
    "    return np.min(x, axis=-1)\n",
    "def maxim(x):\n",
    "    return np.max(x, axis=-1)\n",
    "def argminim(x):\n",
    "    return np.argmin(x, axis=-1)\n",
    "def argmaxim(x):\n",
    "    return np.argmax(x, axis=-1)\n",
    "def rms(x):\n",
    "    return np.sqrt(np.mean(x**2, axis=-1))\n",
    "def abs_diff_signal(x):\n",
    "    return np.sum(np.abs(np.diff(x, axis=-1)), axis=-1)\n",
    "def skewness(x):\n",
    "    return stats.skew(x, axis=-1)\n",
    "def kurtosis(x):\n",
    "    return stats.kurtosis(x, axis=-1)\n",
    "def concatenate_features(x):\n",
    "    return np.concatenate((mean(x), std(x), ptp(x), var(x), minim(x), maxim(x), argmaxim(x), \n",
    "                           argminim(x), rms(x), abs_diff_signal(x), skewness(x), kurtosis(x)), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec1b6fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1596, 276)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = []\n",
    "for d in data_array:\n",
    "    features.append(concatenate_features(d))\n",
    "    \n",
    "features_array = np.array(features)\n",
    "features_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c4e69b",
   "metadata": {},
   "source": [
    "## Visualize features per channel and per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "101d800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info for channel 23: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1596, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get features of specific channel\n",
    "def features_by_channel(features_array):\n",
    "    features_channel = []\n",
    "    MAX_CHANNELS = 23\n",
    "    for i in range(MAX_CHANNELS):\n",
    "        array = features_array[:, i*12:(i+1)*12];\n",
    "        features_channel.append(pd.DataFrame(array, columns=['Mean', 'Std', 'Ptp', 'Var', 'Minimo', 'Maxim', 'Arg Max', 'Arg Min', 'RMS', 'ABS DIFF', 'skewness', 'kurtosis']))\n",
    "    return features_channel\n",
    "\n",
    "features_channel = features_by_channel(features_array)\n",
    "print('Info for channel 23: ')\n",
    "\n",
    "# Shape of features_channel=(23, 1596, 12)\n",
    "features_channel[22].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17252935",
   "metadata": {},
   "source": [
    "# Preprocessing and labelling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1d04223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serching for patient 01 files at /home/guisoares/soares_repo/SCC0276-Machine-Learning/chb01 ...\n",
      "Found 4 files\n"
     ]
    }
   ],
   "source": [
    "# patient_folder = Path.cwd()/'chbmit'/'chb{}'.format(str(patient_id).zfill(2))\n",
    "# DATASET_PATH = \n",
    "DATASET_PATH = os.getcwd() #ONLY FOR TESTING\n",
    "\n",
    "patient_id = 1\n",
    "patient_id_str = str(patient_id).zfill(2)\n",
    "\n",
    "patient_folder = os.path.join(DATASET_PATH, f'chb{patient_id_str}')\n",
    "print(f\"Serching for patient {patient_id_str} files at {patient_folder} ...\")\n",
    "patient_files = glob(os.path.join(patient_folder, f'*.edf'))\n",
    "print(f\"Found {len(patient_files)} files\")\n",
    "summary_path = os.path.join(patient_folder, f'chb{patient_id_str}-summary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a014a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ SUMMARY (TODO: automate this)\n",
    "seizures_dict = {\"chb01_03\": [[2996, 3036]],\n",
    "                \"chb01_04\": [[1467, 1494]],\n",
    "                \"chb01_15\": [[1732, 1772]],\n",
    "                \"chb01_16\": [[1015, 1066]],\n",
    "                \"chb01_18\": [[1720, 1810]],\n",
    "                \"chb01_21\": [[327, 420]],\n",
    "                \"chb01_26\": [[1862, 1963]],\n",
    "                \n",
    "                \"chb02_16\": [[130, 212]],\n",
    "\n",
    "                \"chb05_06\": [[417, 532]], \n",
    "                \"chb05_13\": [[1086, 1196]],\n",
    "                \"chb05_16\": [[2317, 2413]], \n",
    "                \"chb05_17\": [[2451, 2571]],\n",
    "                \"chb05_22\": [[2348, 2465]],\n",
    "                \n",
    "                \"chb08_02\": [[2670, 2841]], \n",
    "                \"chb08_05\": [[2856, 3046]],\n",
    "                \"chb08_11\": [[2988, 3211]], \n",
    "                \"chb08_13\": [[2417, 2577]],\n",
    "                \"chb08_21\": [[2083, 2347]]}\n",
    "\n",
    "curr_time = 0\n",
    "epoch_time = 10\n",
    "overlap = 5\n",
    "\n",
    "# Divide into epochs\n",
    "labels = []\n",
    "for file_path in patient_files:\n",
    "    # get filename: chbxx_xx.edf\n",
    "    filename = os.path.split(file_path)[1]\n",
    "\n",
    "    # remove .edf staying only chbxx_xx\n",
    "    filename = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # read raw\n",
    "    raw = read_edf_to_raw(file_path)\n",
    "\n",
    "    curr_time = 0\n",
    "    while curr_time <= max(raw.times) + 0.01 - epoch_time:  # max(raw.times) = 3600\n",
    "        epoch_features = []\n",
    "        start_time = curr_time \n",
    "        if start_time < 0.:\n",
    "            start_time = 0.\n",
    "        end_time = curr_time + epoch_time\n",
    "        start, stop = raw.time_as_index([start_time, end_time])\n",
    "        temp = raw[:, start:stop][0]\n",
    "\n",
    "        # start time as ID\n",
    "        # features.append(start_time)\n",
    "\n",
    "        # features\n",
    "        epoch_features.extend(concatenate_features(temp))\n",
    "\n",
    "        # seizure flag for y\n",
    "        aux = []\n",
    "        if filename in seizures_dict:  # if file has seizure\n",
    "            for seizure in seizures_dict[filename]:\n",
    "                if start_time > seizure[0] and start_time < seizure[1]:\n",
    "                    aux.append(1)\n",
    "                elif start_time + epoch_time > seizure[0] and start_time + epoch_time < seizure[1]:\n",
    "                    aux.append(1)\n",
    "                else:\n",
    "                    aux.append(0)\n",
    "        else:    \n",
    "            aux.append(0)\n",
    "\n",
    "        if 1 in aux:\n",
    "            epoch_features.extend([1])\n",
    "        else:\n",
    "            epoch_features.extend([0])\n",
    "\n",
    "        labels.append(epoch_features)\n",
    "\n",
    "        curr_time = curr_time + epoch_time - overlap  \n",
    "        print(\"Section \", str(len(labels)), \"; start: \", start, \" ; stop: \", stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1764e4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2876, 277)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba05c58",
   "metadata": {},
   "source": [
    "### Save to npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f387629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"chb01_10i_5o_wlabel.txt\", np.array(labels), delimiter=\" \")\n",
    "np.save(\"chb01_10i_5o_wlabel\", np.array(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be46f1",
   "metadata": {},
   "source": [
    "### Load npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7df37691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2876, 276)\n",
      "(2876,)\n"
     ]
    }
   ],
   "source": [
    "labels = np.load(\"chb01_10i_5o_wlabel.npy\")\n",
    "x, y = labels[:,:276], labels[:,276]\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97120aa",
   "metadata": {},
   "source": [
    "### Filter features by variance and correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9266e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# # check zero variance features\n",
    "# thresholder = VarianceThreshold(threshold=0)\n",
    "# print(\"Variables Kept after removing features with 0 variance: \", thresholder.fit_transform(x).shape[1])\n",
    "\n",
    "# # highly correlated features\n",
    "# corr = abs(x.corr())\n",
    "# upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n",
    "# cols = [column for column in upper.columns if any(upper[column] < 0.9)]\n",
    "# print(\"Variables Kept after removing features with corr > 0.9: \", len(cols)) \n",
    "\n",
    "# normalize features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f65071",
   "metadata": {},
   "source": [
    "### Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2843da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2876, 276)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "x = preprocessing.normalize(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eae1dd",
   "metadata": {},
   "source": [
    "### Split features and labels in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f587771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test =  train_test_split(x, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03fb7973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 2859, 1.0: 17}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d48b2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd156d1bfb48eb4ee1a6eb8f39a444f48635a09375c4c26039d85d949812f675"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('full')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
