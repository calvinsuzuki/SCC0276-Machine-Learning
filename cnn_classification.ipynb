{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 12:21:55.119325: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/guisoares/catkin_ws/devel/lib:/opt/ros/noetic/lib:/usr/lib/x86_64-linux-gnu/:/home/guisoares/catkin_ws/install/lib/mavlink_sitl_gazebo/plugins\n",
      "2022-07-06 12:21:55.119353: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv1D,Conv2D,MaxPooling2D,Flatten,AveragePooling2D,Input,BatchNormalization,AveragePooling1D\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from utils.dataloader import read_edf_to_raw, calculate_neg_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_article():\n",
    "    # Model creation\n",
    "    model = Sequential()\n",
    "    # model.add(Input(shape=(23,25600,1)))\n",
    "\n",
    "    model.add(Conv1D(4, 5, activation='relu', input_shape=(12800, 23), name='conv1'))\n",
    "    model.add(BatchNormalization(name='batch1'))\n",
    "    model.add(AveragePooling1D(pool_size=2, strides=2, name='pool1'))\n",
    "    # output ~(12800,4)\n",
    "\n",
    "    model.add(Conv1D(4, 6, activation='relu', name='conv2'))\n",
    "    model.add(BatchNormalization(name='batch2'))\n",
    "    model.add(AveragePooling1D(pool_size=4, strides=1,name='pool2'))\n",
    "    # output ~(12800,4)\n",
    "\n",
    "    model.add(Conv1D(10, 4, activation='relu', name='conv3'))\n",
    "    model.add(BatchNormalization(name='batch3'))\n",
    "    model.add(AveragePooling1D(pool_size=2, strides=2, name='pool3'))\n",
    "    # output ~(6400,4)\n",
    "\n",
    "    model.add(Conv1D(10, 4, activation='relu', name='conv4'))\n",
    "    model.add(BatchNormalization(name='batch4'))\n",
    "    model.add(AveragePooling1D(pool_size=2, strides=4, name='pool4'))\n",
    "    # output ~(1200,4)\n",
    "\n",
    "    model.add(Conv1D(15, 2, activation='relu', name='conv5'))\n",
    "    model.add(BatchNormalization(name='batch5'))\n",
    "    model.add(AveragePooling1D(pool_size=2, strides=4, name='pool5'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_customized():\n",
    "    # Model creation\n",
    "    model = Sequential()\n",
    "    # model.add(Input(shape=(23,25600,1)))\n",
    "\n",
    "    model.add(Conv2D(16, 3, activation='relu', padding='same', input_shape=(25600, 23, 1), name='conv1'))\n",
    "    model.add(BatchNormalization(name='batch1'))\n",
    "    model.add(MaxPooling2D(pool_size=(32,2), name='pool1'))\n",
    "\n",
    "    model.add(Conv2D(8, 5, activation='relu', padding='same', name='conv2'))\n",
    "    model.add(BatchNormalization(name='batch2'))\n",
    "    model.add(MaxPooling2D(pool_size=(16,2), name='pool2'))\n",
    "\n",
    "    model.add(Conv2D(4, 7, activation='relu', padding='same', name='conv3'))\n",
    "    model.add(BatchNormalization(name='batch3'))\n",
    "    model.add(MaxPooling2D(pool_size=(8,2), name='pool3'))\n",
    "\n",
    "    model.add(Conv2D(2, 3, activation='relu',  padding='same', name='conv7'))\n",
    "    model.add(BatchNormalization(name='batch7'))\n",
    "    model.add(MaxPooling2D(pool_size=(4,2), name='pool4'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1 (Conv1D)              (None, 12796, 4)          464       \n",
      "                                                                 \n",
      " batch1 (BatchNormalization)  (None, 12796, 4)         16        \n",
      "                                                                 \n",
      " pool1 (AveragePooling1D)    (None, 6398, 4)           0         \n",
      "                                                                 \n",
      " conv2 (Conv1D)              (None, 6393, 4)           100       \n",
      "                                                                 \n",
      " batch2 (BatchNormalization)  (None, 6393, 4)          16        \n",
      "                                                                 \n",
      " pool2 (AveragePooling1D)    (None, 6390, 4)           0         \n",
      "                                                                 \n",
      " conv3 (Conv1D)              (None, 6387, 10)          170       \n",
      "                                                                 \n",
      " batch3 (BatchNormalization)  (None, 6387, 10)         40        \n",
      "                                                                 \n",
      " pool3 (AveragePooling1D)    (None, 3193, 10)          0         \n",
      "                                                                 \n",
      " conv4 (Conv1D)              (None, 3190, 10)          410       \n",
      "                                                                 \n",
      " batch4 (BatchNormalization)  (None, 3190, 10)         40        \n",
      "                                                                 \n",
      " pool4 (AveragePooling1D)    (None, 798, 10)           0         \n",
      "                                                                 \n",
      " conv5 (Conv1D)              (None, 797, 15)           315       \n",
      "                                                                 \n",
      " batch5 (BatchNormalization)  (None, 797, 15)          60        \n",
      "                                                                 \n",
      " pool5 (AveragePooling1D)    (None, 199, 15)           0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2985)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 50)                149300    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 151,452\n",
      "Trainable params: 151,366\n",
      "Non-trainable params: 86\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model_article()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(learning_rate=0.1)\n",
    "# Create the loss function object using the wrapper function above\n",
    "# spec_loss = specificity_loss_wrapper()\n",
    "model.compile(\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "            optimizer=opt, \n",
    "            metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5), tf.keras.metrics.SensitivityAtSpecificity(0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files:\n",
      "['/media/guisoares/guisoares-ext-hdd/Datasets/chb-mit-segments/1/train/chb01_segments.npz', '/media/guisoares/guisoares-ext-hdd/Datasets/chb-mit-segments/1/train/chb02_segments.npz', '/media/guisoares/guisoares-ext-hdd/Datasets/chb-mit-segments/1/train/chb08_segments.npz']\n",
      "Testing files:\n",
      "['/media/guisoares/guisoares-ext-hdd/Datasets/chb-mit-segments/1/test/chb05_segments.npz']\n"
     ]
    }
   ],
   "source": [
    "NEW_DATASET_PATH = \"/media/guisoares/guisoares-ext-hdd/Datasets/chb-mit-segments/1\"\n",
    "\n",
    "train_files = glob.glob(os.path.join(NEW_DATASET_PATH,'train',\"*.npz\"))\n",
    "test_files = glob.glob(os.path.join(NEW_DATASET_PATH,'test',\"*.npz\"))\n",
    "\n",
    "print(\"Training files:\")\n",
    "print(train_files)\n",
    "print(\"Testing files:\")\n",
    "print(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test arrays to memory\n",
    "x_test = []\n",
    "y_test = []\n",
    "for file in test_files:\n",
    "    test_arrays = np.load(file)\n",
    "    x_test.append(test_arrays['arr_0'])\n",
    "    y_test.append(test_arrays['arr_1'])\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = x_test.reshape((x_test.shape[0]*x_test.shape[1], x_test.shape[2], x_test.shape[3]))\n",
    "x_test = np.swapaxes(x_test, 1, 2)\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "y_test = y_test.reshape((y_test.shape[0]*y_test.shape[1], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negatives: 9, Positives: 4\n"
     ]
    }
   ],
   "source": [
    "def calculate_neg_pos(y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    dic = dict(zip(unique, counts))\n",
    "    neg = dic[0]\n",
    "    pos = dic[1]\n",
    "    return neg, pos\n",
    "\n",
    "neg_test, pos_test = calculate_neg_pos(y_test)\n",
    "print(f\"Negatives: {neg_test}, Positives: {pos_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train :{0: 7, 1: 3} 10/10 - 4s - loss: 0.6546 - binary_accuracy: 0.5000 - sensitivity_at_specificity_3: 0.3333 - val_loss: 16.1904 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 4s/epoch - 355ms/step\n",
      "Epoch: 1 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.9146 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 4.9822 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 342ms/epoch - 34ms/step\n",
      "Epoch: 1 Train :{0: 14, 1: 6} 20/20 - 0s - loss: 0.6445 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.1667 - val_loss: 11.3126 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 460ms/epoch - 23ms/step\n",
      "Epoch: 2 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6818 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 6.5488 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 300ms/epoch - 30ms/step\n",
      "Epoch: 2 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6474 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 6.4184 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 299ms/epoch - 30ms/step\n",
      "Epoch: 2 Train :{0: 14, 1: 6} 20/20 - 1s - loss: 0.6412 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.5000 - val_loss: 4.9151 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 672ms/epoch - 34ms/step\n",
      "Epoch: 3 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6349 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 6.3962 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 323ms/epoch - 32ms/step\n",
      "Epoch: 3 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6305 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 7.6388 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 324ms/epoch - 32ms/step\n",
      "Epoch: 3 Train :{0: 14, 1: 6} 20/20 - 0s - loss: 0.6283 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.5000 - val_loss: 9.7896 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 461ms/epoch - 23ms/step\n",
      "Epoch: 4 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6293 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.3333 - val_loss: 8.4316 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 345ms/epoch - 34ms/step\n",
      "Epoch: 4 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6286 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.3333 - val_loss: 8.6763 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 386ms/epoch - 39ms/step\n",
      "Epoch: 4 Train :{0: 14, 1: 6} 20/20 - 1s - loss: 0.6266 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.3333 - val_loss: 8.8177 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 567ms/epoch - 28ms/step\n",
      "Epoch: 5 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6253 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.3333 - val_loss: 10.2656 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 330ms/epoch - 33ms/step\n",
      "Epoch: 5 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6260 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 10.8331 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 311ms/epoch - 31ms/step\n",
      "Epoch: 5 Train :{0: 14, 1: 6} 20/20 - 1s - loss: 0.6247 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.3333 - val_loss: 12.5857 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 587ms/epoch - 29ms/step\n",
      "Epoch: 6 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6253 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 13.2091 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 321ms/epoch - 32ms/step\n",
      "Epoch: 6 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6250 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 14.4953 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 321ms/epoch - 32ms/step\n",
      "Epoch: 6 Train :{0: 14, 1: 6} 20/20 - 1s - loss: 0.6246 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 13.4994 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 512ms/epoch - 26ms/step\n",
      "Epoch: 7 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6242 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 13.9882 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 325ms/epoch - 32ms/step\n",
      "Epoch: 7 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6243 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 14.3889 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 332ms/epoch - 33ms/step\n",
      "Epoch: 7 Train :{0: 14, 1: 6} 20/20 - 1s - loss: 0.6234 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.1667 - val_loss: 16.4190 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 502ms/epoch - 25ms/step\n",
      "Epoch: 8 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6234 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 15.2247 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 404ms/epoch - 40ms/step\n",
      "Epoch: 8 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6239 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 16.0594 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 364ms/epoch - 36ms/step\n",
      "Epoch: 8 Train :{0: 14, 1: 6} 20/20 - 1s - loss: 0.6231 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.1667 - val_loss: 20.4689 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 506ms/epoch - 25ms/step\n",
      "Epoch: 9 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6236 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 25.7232 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 314ms/epoch - 31ms/step\n",
      "Epoch: 9 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6236 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 27.8047 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 402ms/epoch - 40ms/step\n",
      "Epoch: 9 Train :{0: 14, 1: 6} 20/20 - 1s - loss: 0.6221 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.1667 - val_loss: 34.2614 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 564ms/epoch - 28ms/step\n",
      "Epoch: 10 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6234 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 40.9801 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 376ms/epoch - 38ms/step\n",
      "Epoch: 10 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6232 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 42.7577 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 497ms/epoch - 50ms/step\n",
      "Epoch: 10 Train :{0: 14, 1: 6} 20/20 - 1s - loss: 0.6228 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 52.5194 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 699ms/epoch - 35ms/step\n",
      "Epoch: 11 Train :{0: 7, 1: 3} 10/10 - 0s - loss: 0.6228 - binary_accuracy: 0.7000 - sensitivity_at_specificity_3: 0.0000e+00 - val_loss: 51.6550 - val_binary_accuracy: 0.6923 - val_sensitivity_at_specificity_3: 0.0000e+00 - 419ms/epoch - 42ms/step\n",
      "Epoch: 11 Train :{0: 7, 1: 3} "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38397/4267950198.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train :{dict(zip(unique, counts))} \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'binary_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/full/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/full/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1443\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1445\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1446\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/full/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/full/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1750\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/full/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/full/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[0;32m~/.virtualenvs/full/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/full/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    719\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 721\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/full/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3407\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3409\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3410\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3411\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# callback = tf.keras.callbacks.EarlyStopping(monitor = 'accuracy', min_delta=0.05)\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    for file in train_files:\n",
    "\n",
    "        # y = np.load('signals/chb01/labels.npy')\n",
    "        # x = np.load('signals/chb01/signals.npy') \n",
    "\n",
    "        arrays = np.load(file)\n",
    "        x_train = arrays['arr_0']\n",
    "        y_train = arrays['arr_1']\n",
    "        x_train = np.swapaxes(x_train, 1, 2) #swap axis: N,23,25600 -> N,25600, 23\n",
    "        # x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, shuffle=True)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} \", end = \"\")\n",
    "        unique, counts = np.unique(y_train, return_counts=True)\n",
    "        print(f\"Train :{dict(zip(unique, counts))} \", end=\"\")\n",
    "\n",
    "        history = model.fit(x_train, y_train, batch_size=1, epochs=1, validation_data=(x_test,y_test), verbose=2)\n",
    "\n",
    "        if history.history['binary_accuracy'][0] > 0.99:\n",
    "            break\n",
    "    if history.history['binary_accuracy'][0] > 0.99:\n",
    "        break\n",
    "    \n",
    "        # train_metrics = model.train_on_batch(x_train, y_train, return_dict=True)\n",
    "        # test_metrics = model.test_on_batch(x_test, y_test, return_dict=True)\n",
    "\n",
    "        # print(train_metrics, test_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 98ms/step\n",
      "[[0.00143324]\n",
      " [0.00143306]\n",
      " [0.0014334 ]\n",
      " [0.00143323]\n",
      " [0.00143336]\n",
      " [0.00143315]\n",
      " [0.00143329]\n",
      " [0.00143332]\n",
      " [0.00143399]\n",
      " [0.00143367]]\n",
      "{0: 10}\n",
      "{0: 7, 1: 3}\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_train)\n",
    "print(y_pred)\n",
    "y_pred = np.where(y_pred < 0.5, 0, 1)\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def compute_binary_specificity(y_pred, y_true):\n",
    "    \"\"\"Compute the confusion matrix for a set of predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred   : predicted values for a batch if samples (must be binary: 0 or 1)\n",
    "    y_true   : correct values for the set of samples used (must be binary: 0 or 1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : the specificity\n",
    "    \"\"\"\n",
    "\n",
    "    # check_binary(K.eval(y_true))    # must check that input values are 0 or 1\n",
    "    # check_binary(K.eval(y_pred))    # \n",
    "\n",
    "    TN = np.logical_and(K.eval(y_true) == 0, K.eval(y_pred) == 0)\n",
    "    FP = np.logical_and(K.eval(y_true) == 0, K.eval(y_pred) == 1)\n",
    "\n",
    "    # as Keras Tensors\n",
    "    TN = K.sum(K.variable(TN))\n",
    "    FP = K.sum(K.variable(FP))\n",
    "\n",
    "    specificity = TN / (TN + FP + K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "def specificity_loss_wrapper():\n",
    "    \"\"\"A wrapper to create and return a function which computes the specificity loss, as (1 - specificity)\n",
    "\n",
    "    \"\"\"\n",
    "    # Define the function for your loss\n",
    "    def specificity_loss(y_true, y_pred):\n",
    "        return 1.0 - compute_binary_specificity(y_true, y_pred)\n",
    "\n",
    "    return specificity_loss    # we return this function object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('full')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd156d1bfb48eb4ee1a6eb8f39a444f48635a09375c4c26039d85d949812f675"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
