{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 12:21:55.119325: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/guisoares/catkin_ws/devel/lib:/opt/ros/noetic/lib:/usr/lib/x86_64-linux-gnu/:/home/guisoares/catkin_ws/install/lib/mavlink_sitl_gazebo/plugins\n",
      "2022-07-06 12:21:55.119353: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import glob\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv1D,Conv2D,MaxPooling2D,Flatten,AveragePooling2D,Input,BatchNormalization,AveragePooling1D\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from utils.dataloader import read_edf_to_raw, calculate_neg_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_article():\n",
    "    # Model creation\n",
    "    model = Sequential()\n",
    "    # model.add(Input(shape=(23,25600,1)))\n",
    "\n",
    "    model.add(Conv1D(4, 5, activation='relu', input_shape=(12800, 23), name='conv1'))\n",
    "    model.add(BatchNormalization(name='batch1'))\n",
    "    model.add(AveragePooling1D(pool_size=2, strides=2, name='pool1'))\n",
    "    # output ~(12800,4)\n",
    "\n",
    "    model.add(Conv1D(4, 6, activation='relu', name='conv2'))\n",
    "    model.add(BatchNormalization(name='batch2'))\n",
    "    model.add(AveragePooling1D(pool_size=4, strides=1,name='pool2'))\n",
    "    # output ~(12800,4)\n",
    "\n",
    "    model.add(Conv1D(10, 4, activation='relu', name='conv3'))\n",
    "    model.add(BatchNormalization(name='batch3'))\n",
    "    model.add(AveragePooling1D(pool_size=2, strides=2, name='pool3'))\n",
    "    # output ~(6400,4)\n",
    "\n",
    "    model.add(Conv1D(10, 4, activation='relu', name='conv4'))\n",
    "    model.add(BatchNormalization(name='batch4'))\n",
    "    model.add(AveragePooling1D(pool_size=2, strides=4, name='pool4'))\n",
    "    # output ~(1200,4)\n",
    "\n",
    "    model.add(Conv1D(15, 2, activation='relu', name='conv5'))\n",
    "    model.add(BatchNormalization(name='batch5'))\n",
    "    model.add(AveragePooling1D(pool_size=2, strides=4, name='pool5'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_customized():\n",
    "    # Model creation\n",
    "    model = Sequential()\n",
    "    # model.add(Input(shape=(23,25600,1)))\n",
    "\n",
    "    model.add(Conv2D(16, 3, activation='relu', padding='same', input_shape=(25600, 23, 1), name='conv1'))\n",
    "    model.add(BatchNormalization(name='batch1'))\n",
    "    model.add(MaxPooling2D(pool_size=(32,2), name='pool1'))\n",
    "\n",
    "    model.add(Conv2D(8, 5, activation='relu', padding='same', name='conv2'))\n",
    "    model.add(BatchNormalization(name='batch2'))\n",
    "    model.add(MaxPooling2D(pool_size=(16,2), name='pool2'))\n",
    "\n",
    "    model.add(Conv2D(4, 7, activation='relu', padding='same', name='conv3'))\n",
    "    model.add(BatchNormalization(name='batch3'))\n",
    "    model.add(MaxPooling2D(pool_size=(8,2), name='pool3'))\n",
    "\n",
    "    model.add(Conv2D(2, 3, activation='relu',  padding='same', name='conv7'))\n",
    "    model.add(BatchNormalization(name='batch7'))\n",
    "    model.add(MaxPooling2D(pool_size=(4,2), name='pool4'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 12:21:58.224551: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/guisoares/catkin_ws/devel/lib:/opt/ros/noetic/lib:/usr/lib/x86_64-linux-gnu/:/home/guisoares/catkin_ws/install/lib/mavlink_sitl_gazebo/plugins\n",
      "2022-07-06 12:21:58.224582: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-06 12:21:58.224606: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (guisoares-Aspire-A315-53): /proc/driver/nvidia/version does not exist\n",
      "2022-07-06 12:21:58.225091: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1 (Conv1D)              (None, 12796, 4)          464       \n",
      "                                                                 \n",
      " batch1 (BatchNormalization)  (None, 12796, 4)         16        \n",
      "                                                                 \n",
      " pool1 (AveragePooling1D)    (None, 6398, 4)           0         \n",
      "                                                                 \n",
      " conv2 (Conv1D)              (None, 6393, 4)           100       \n",
      "                                                                 \n",
      " batch2 (BatchNormalization)  (None, 6393, 4)          16        \n",
      "                                                                 \n",
      " pool2 (AveragePooling1D)    (None, 6390, 4)           0         \n",
      "                                                                 \n",
      " conv3 (Conv1D)              (None, 6387, 10)          170       \n",
      "                                                                 \n",
      " batch3 (BatchNormalization)  (None, 6387, 10)         40        \n",
      "                                                                 \n",
      " pool3 (AveragePooling1D)    (None, 3193, 10)          0         \n",
      "                                                                 \n",
      " conv4 (Conv1D)              (None, 3190, 10)          410       \n",
      "                                                                 \n",
      " batch4 (BatchNormalization)  (None, 3190, 10)         40        \n",
      "                                                                 \n",
      " pool4 (AveragePooling1D)    (None, 798, 10)           0         \n",
      "                                                                 \n",
      " conv5 (Conv1D)              (None, 797, 15)           315       \n",
      "                                                                 \n",
      " batch5 (BatchNormalization)  (None, 797, 15)          60        \n",
      "                                                                 \n",
      " pool5 (AveragePooling1D)    (None, 199, 15)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2985)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                149300    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 151,452\n",
      "Trainable params: 151,366\n",
      "Non-trainable params: 86\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model_article()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "# Create the loss function object using the wrapper function above\n",
    "# spec_loss = specificity_loss_wrapper()\n",
    "model.compile(\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "            optimizer=opt, \n",
    "            metrics=['accuracy', tf.keras.metrics.SensitivityAtSpecificity(0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files:\n",
      "['/media/guisoares/guisoares-ext-hdd/Datasets/chb-mit-segments/1/train/chb01_segments.npz', '/media/guisoares/guisoares-ext-hdd/Datasets/chb-mit-segments/1/train/chb02_segments.npz', '/media/guisoares/guisoares-ext-hdd/Datasets/chb-mit-segments/1/train/chb08_segments.npz']\n",
      "Testing files:\n",
      "['/media/guisoares/guisoares-ext-hdd/Datasets/chb-mit-segments/1/test/chb05_segments.npz']\n"
     ]
    }
   ],
   "source": [
    "NEW_DATASET_PATH = \"/media/guisoares/guisoares-ext-hdd/Datasets/chb-mit-segments/1\"\n",
    "\n",
    "train_files = glob.glob(os.path.join(NEW_DATASET_PATH,'train',\"*.npz\"))\n",
    "test_files = glob.glob(os.path.join(NEW_DATASET_PATH,'test',\"*.npz\"))\n",
    "\n",
    "print(\"Training files:\")\n",
    "print(train_files)\n",
    "print(\"Testing files:\")\n",
    "print(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test arrays to memory\n",
    "x_test = []\n",
    "y_test = []\n",
    "for file in test_files:\n",
    "    test_arrays = np.load(file)\n",
    "    x_test.append(test_arrays['arr_0'])\n",
    "    y_test.append(test_arrays['arr_1'])\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = x_test.reshape((x_test.shape[0]*x_test.shape[1], x_test.shape[2], x_test.shape[3]))\n",
    "x_test = np.swapaxes(x_test, 1, 2)\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "y_test = y_test.reshape((y_test.shape[0]*y_test.shape[1], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negatives: 9, Positives: 4\n"
     ]
    }
   ],
   "source": [
    "def calculate_neg_pos(y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    dic = dict(zip(unique, counts))\n",
    "    neg = dic[0]\n",
    "    pos = dic[1]\n",
    "    return neg, pos\n",
    "\n",
    "neg_test, pos_test = calculate_neg_pos(y_test)\n",
    "print(f\"Negatives: {neg_test}, Positives: {pos_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train :{0: 7, 1: 3} 1/1 - 5s - loss: 0.9618 - accuracy: 0.3000 - sensitivity_at_specificity: 0.3333 - val_loss: 2.5711 - val_accuracy: 0.3077 - val_sensitivity_at_specificity: 0.0000e+00 - 5s/epoch - 5s/step\n",
      "Epoch: 1 Train :{0: 7, 1: 3} 1/1 - 0s - loss: 4.3758 - accuracy: 0.7000 - sensitivity_at_specificity: 1.0000 - val_loss: 1.4332 - val_accuracy: 0.3077 - val_sensitivity_at_specificity: 0.0000e+00 - 324ms/epoch - 324ms/step\n",
      "Epoch: 1 Train :{0: 14, 1: 6} 1/1 - 0s - loss: 0.9722 - accuracy: 0.4500 - sensitivity_at_specificity: 0.8333 - val_loss: 2.5811 - val_accuracy: 0.3077 - val_sensitivity_at_specificity: 0.0000e+00 - 417ms/epoch - 417ms/step\n",
      "Epoch: 2 Train :{0: 7, 1: 3} 1/1 - 0s - loss: 0.0175 - accuracy: 1.0000 - sensitivity_at_specificity: 1.0000 - val_loss: 2.1661 - val_accuracy: 0.3077 - val_sensitivity_at_specificity: 0.0000e+00 - 283ms/epoch - 283ms/step\n"
     ]
    }
   ],
   "source": [
    "# callback = tf.keras.callbacks.EarlyStopping(monitor = 'accuracy', min_delta=0.05)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for file in train_files:\n",
    "\n",
    "        # y = np.load('signals/chb01/labels.npy')\n",
    "        # x = np.load('signals/chb01/signals.npy') \n",
    "\n",
    "        arrays = np.load(file)\n",
    "        x_train = arrays['arr_0']\n",
    "        y_train = arrays['arr_1']\n",
    "        x_train = np.swapaxes(x_train, 1, 2) #swap axis: N,23,25600 -> N,25600, 23\n",
    "        # x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, shuffle=True)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} \", end = \"\")\n",
    "        unique, counts = np.unique(y_train, return_counts=True)\n",
    "        print(f\"Train :{dict(zip(unique, counts))} \", end=\"\")\n",
    "\n",
    "        history = model.fit(x_train, y_train, batch_size=32, epochs=1, validation_data=(x_test,y_test), verbose=2)\n",
    "\n",
    "        if history.history['accuracy'][0] > 0.99:\n",
    "            break\n",
    "    if history.history['accuracy'][0] > 0.99:\n",
    "        break\n",
    "    \n",
    "        # train_metrics = model.train_on_batch(x_train, y_train, return_dict=True)\n",
    "        # test_metrics = model.test_on_batch(x_test, y_test, return_dict=True)\n",
    "\n",
    "        # print(train_metrics, test_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 360ms/step\n",
      "{1: 10}\n",
      "{0: 7, 1: 3}\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_train[:])\n",
    "y_pred = np.where(y_pred < 0.5, 0, 1)\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def compute_binary_specificity(y_pred, y_true):\n",
    "    \"\"\"Compute the confusion matrix for a set of predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred   : predicted values for a batch if samples (must be binary: 0 or 1)\n",
    "    y_true   : correct values for the set of samples used (must be binary: 0 or 1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : the specificity\n",
    "    \"\"\"\n",
    "\n",
    "    # check_binary(K.eval(y_true))    # must check that input values are 0 or 1\n",
    "    # check_binary(K.eval(y_pred))    # \n",
    "\n",
    "    TN = np.logical_and(K.eval(y_true) == 0, K.eval(y_pred) == 0)\n",
    "    FP = np.logical_and(K.eval(y_true) == 0, K.eval(y_pred) == 1)\n",
    "\n",
    "    # as Keras Tensors\n",
    "    TN = K.sum(K.variable(TN))\n",
    "    FP = K.sum(K.variable(FP))\n",
    "\n",
    "    specificity = TN / (TN + FP + K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "def specificity_loss_wrapper():\n",
    "    \"\"\"A wrapper to create and return a function which computes the specificity loss, as (1 - specificity)\n",
    "\n",
    "    \"\"\"\n",
    "    # Define the function for your loss\n",
    "    def specificity_loss(y_true, y_pred):\n",
    "        return 1.0 - compute_binary_specificity(y_true, y_pred)\n",
    "\n",
    "    return specificity_loss    # we return this function object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('full')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd156d1bfb48eb4ee1a6eb8f39a444f48635a09375c4c26039d85d949812f675"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
